Описание тестируемых приложений

### A) REST API (FastAPI)

#### Архитектура и используемые технологии

Тестируемый сервис реализован в виде REST API с использованием фреймворка **FastAPI**.  
Взаимодействие между клиентом и сервером осуществляется по протоколу HTTP, а данные передаются в формате **JSON**.

Нагрузочное тестирование REST-сервиса выполнялось в режиме *read-only* (только GET-запросы), поскольку основной целью эксперимента является сравнение производительности операций чтения и ресурсоёмких вычислительных запросов.  
Описание сценариев нагрузки и логики поведения пользователей приведено в файле `locustfile_rest_simple.py`.

#### Эндпоинты, участвующие в тестировании

В ходе нагрузочного тестирования использовались следующие эндпоинты:

- `GET /terms` — получение списка терминов (лёгкая операция);
- `GET /terms/{term}` — получение конкретного термина (лёгкая операция);
- `GET /graph` — получение полного семантического графа терминов (тяжёлая операция).

Все перечисленные эндпоинты используются в сценариях, описанных в файле `locustfile_rest_simple.py`.

#### Использование базы данных

Со стороны инструмента нагрузочного тестирования Locust использование базы данных напрямую не фиксируется.  
Функционально REST-сервис возвращает список терминов и структуру графа в формате JSON, что позволяет предположить использование внутреннего хранилища данных (например, базы данных SQLite, файлового хранилища или хранения в памяти).

В рамках данного отчёта база данных рассматривается как **внутренний компонент сервиса**, детали реализации которого не влияют на измеряемые внешние метрики производительности.

#### Возвращаемые данные

- Эндпоинт `/terms` возвращает массив терминов, каждый элемент которого содержит поле `term` либо `id`;
- Эндпоинт `/graph` возвращает структуру семантического графа, включающую узлы (`nodes`) и связи (`edges`).

Данные структуры используются для анализа сложности запросов и их влияния на производительность сервиса.

#### Данные, необходимые для добавления термина

В текущей конфигурации REST-нагрузочного теста операции записи не используются.  
Однако типичная операция добавления термина в глоссарий требует передачи следующих данных:

- `term` — наименование термина;
- `description` — описание термина.

Операция добавления рассматривается концептуально в рамках сравнения REST и gRPC, однако в данной версии эксперимента используется профиль только на чтение.

---

### B) gRPC сервис (RPC + Protocol Buffers)

#### Архитектура и используемые технологии

Второй тестируемый сервис реализован с использованием технологии **gRPC**, основанной на подходе удалённого вызова процедур (RPC).  
Для сериализации данных применяется бинарный формат **Protocol Buffers**, обеспечивающий компактное представление данных и снижение накладных расходов при передаче сообщений.

Взаимодействие между клиентом и сервером осуществляется через строго типизированные RPC-методы.

#### Методы сервиса

В gRPC-сервисе реализованы следующие методы:

- `GetTerm` — получение термина по идентификатору `term_id` (лёгкая операция);
- `SearchTerms` — поиск терминов по запросу (более ресурсоёмкая операция);
- `ListTerms` — получение списка терминов;
- `GetTermRelations` — получение связей между терминами (тяжёлая операция);
- `AddTerm` — добавление нового термина (операция записи).

Таким образом, сервис содержит методы с различной логикой и уровнем трудоёмкости, что позволяет корректно сравнивать его производительность с REST API.

#### Использование базы данных

Как и в случае с REST-сервисом, наличие и тип базы данных определяются реализацией gRPC-приложения.  
В рамках данного отчёта фиксируется, что сервис управляет коллекцией терминов и их связями, а хранение данных может быть реализовано как в памяти, так и с использованием базы данных.

При нагрузочном тестировании измеряются исключительно внешние метрики: время отклика, пропускная способность и количество ошибок.

#### Возвращаемые данные

Структуры данных, используемые gRPC-сервисом, описаны в файле `glossary_pb2`:

- `Term` — содержит поля `term`, `description`, `sources`, `created_at`, `updated_at`;
- `SearchTermsResponse` — включает список `terms`, общее количество результатов (`total_count`) и смещение (`offset`);
- `GetTermRelationsResponse` — содержит список связей (`relations`), каждая из которых включает `source_term`, `target_term` и `relation_type`.

#### Данные, необходимые для AddTerm (операция записи)

Для добавления нового термина через gRPC-сервис требуется передача следующих данных:

- `term` — строка с названием термина;
- `description` — строка с описанием;
- `sources` — список строк, содержащих источники информации.

  Пункт 1. Настройки тестовой среды
1.1 Аппаратные ресурсы (CPU, RAM, сеть)

Нагрузочное тестирование REST и gRPC сервисов проводилось на локальном вычислительном стенде, представляющем собой одну машину. Использование локального стенда позволяет исключить влияние нестабильности внешней сети и сосредоточиться на сравнении производительности самих сервисов.

Основные аппаратные характеристики тестового стенда:

Процессор (CPU): указывается на основе вывода команды lscpu;

Оперативная память (RAM): указывается на основе вывода команды free -h;

Сеть: локальное соединение (localhost), влияние внешней сети на результаты тестирования минимально.

Для фиксации аппаратных параметров использовались следующие команды:

lscpu
free -h
ip a

1.2 Архитектура тестового стенда

Тестовый стенд включает следующие компоненты:

REST API, реализованный на FastAPI и запущенный локально, принимающий HTTP-запросы по адресу
http://localhost:8000;

gRPC-сервис, реализованный с использованием Protocol Buffers и запущенный локально, принимающий RPC-запросы по адресу
localhost:50051;

инструмент нагрузочного тестирования Locust, запускаемый отдельным процессом и генерирующий нагрузку на оба сервиса.

Все компоненты размещены на одной машине и функционируют независимо друг от друга, что обеспечивает сопоставимые условия тестирования для REST и gRPC сервисов.

1.3 Версия Locust и программная среда

Для проведения нагрузочного тестирования использовался инструмент Locust, версия которого зафиксирована в файле requirements.txt проекта и может быть дополнительно проверена с помощью команды:

locust -V


Программная среда выполнения тестов и сервисов:

Язык программирования: Python версии 3.11;

Инструмент нагрузочного тестирования: Locust версии 2.32.3;

Среда выполнения сервисов: локальный запуск (процессы или контейнеры).

Использование одинаковой версии интерпретатора Python и одного инструмента нагрузочного тестирования для обоих сервисов позволяет обеспечить корректность сравнительного анализа.

1.4 Дополнительные инструменты мониторинга

Во время проведения нагрузочного тестирования использовались стандартные средства мониторинга системных ресурсов:

docker stats — при запуске сервисов в контейнерах;

htop или top — для наблюдения за загрузкой процессора;

free -h — для контроля использования оперативной памяти.

Указанные инструменты применялись для визуального контроля состояния системы и выявления возможных перегрузок во время выполнения тестов.
Тестовые сценарии нагрузочного тестирования

Для анализа производительности REST и gRPC сервисов были разработаны несколько сценариев нагрузочного тестирования, отличающихся характером и интенсивностью нагрузки. Каждый сценарий моделирует отдельный режим работы системы и позволяет оценить её поведение в различных условиях.

2.1 Лёгкая нагрузка (Sanity check)

Цель сценария:
Проверка корректности работы сервисов и правильности настройки нагрузочного тестирования.

Логика поведения пользователя (task flow):

получение списка терминов;

получение конкретного термина;

выполнение только лёгких операций чтения.

Конфигурация нагрузки:

количество пользователей: 1–5;

скорость подъёма нагрузки (spawn rate): 1 пользователь/сек;

длительность теста: 1–2 минуты.

Ожидания перед запуском (гипотезы):
Ожидалось, что оба сервиса будут работать стабильно, с минимальным временем отклика и без ошибок. Данный сценарий используется для предварительной проверки и не предназначен для выявления пределов производительности.

Фрагмент тестового кода (REST):

@task(35)
def view_all_terms(self):
    self.client.get("/terms")

2.2 Рабочая нагрузка (Normal load)

Цель сценария:
Моделирование типичного использования сервисов в штатном режиме.

Логика поведения пользователя:

получение списка терминов;

запрос конкретных терминов;

периодическое выполнение более ресурсоёмких операций (получение графа или связей).

Для повышения реалистичности используются веса задач и паузы между запросами.

Конфигурация нагрузки:

количество пользователей: 20–50;

скорость подъёма нагрузки (spawn rate): 5 пользователей/сек;

длительность теста: 3–10 минут.

Ожидания перед запуском:
Предполагалось, что при рабочей нагрузке время отклика увеличится по сравнению с лёгкой нагрузкой, однако сервисы сохранят стабильную работу без существенного роста количества ошибок.

Фрагмент тестового кода (gRPC):

@task(35)
def list_all_terms(self):
    request = glossary_pb2.ListTermsRequest()
    self.client.stub.ListTerms(request, timeout=10)

2.3 Стресс-тест (Stress test)

Цель сценария:
Определение пределов производительности сервисов и выявление момента начала деградации.

Логика поведения пользователя:

выполнение запросов с минимальными паузами;

высокая частота обращений к основным методам;

активное использование ресурсоёмких операций.

Конфигурация нагрузки:

количество пользователей: 100–300;

скорость подъёма нагрузки (spawn rate): 10–30 пользователей/сек;

длительность теста: 5–15 минут.

Ожидания перед запуском:
Ожидалось, что при достижении определённого уровня нагрузки начнёт наблюдаться резкий рост времени отклика и появление ошибок, что будет свидетельствовать о достижении пределов производительности сервисов.

Фрагмент тестового кода (REST):

class StressUser(HttpUser):
    wait_time = between(0.1, 0.5)

    @task(50)
    def rapid_reads(self):
        self.client.get("/terms")

2.4 Тест на стабильность (Stability test)

Цель сценария:
Проверка устойчивости сервисов при длительной нагрузке и выявление возможной деградации производительности со временем.

Логика поведения пользователя:

повторяющиеся запросы к основным эндпоинтам;

умеренная интенсивность нагрузки;

стабильные паузы между запросами.

Конфигурация нагрузки:

количество пользователей: 30–100;

скорость подъёма нагрузки (spawn rate): 2–5 пользователей/сек;

длительность теста: 10–60 минут.

Ожидания перед запуском:
Ожидалось, что при длительной стабильной нагрузке время отклика сервисов не будет существенно увеличиваться, а количество ошибок останется на низком уровне.
Результаты нагрузочного тестирования

В данном разделе представлены результаты нагрузочного тестирования REST и gRPC сервисов, полученные с помощью инструмента Locust. Для каждого сценария фиксировались основные метрики производительности, после чего выполнялся анализ поведения сервисов под различной нагрузкой.

3.1 Основные метрики производительности

В ходе тестирования для каждого сценария были зафиксированы следующие метрики:

RPS (Requests Per Second) — количество запросов в секунду;

Throughput — фактическая пропускная способность сервиса;

Среднее время ответа (Average Response Time);

p95 / p99 latency — 95-й и 99-й перцентили времени отклика;

Количество ошибок (HTTP 5xx, тайм-ауты, ошибки соединения).

Сбор метрик осуществлялся средствами Locust.
Для REST-сервиса использовались стандартные возможности HttpUser, для gRPC-сервиса — ручная регистрация запросов через механизм events.request.fire, что позволило корректно учитывать RPC-вызовы в отчётах Locust.

Результаты тестирования сохранялись в виде HTML-отчётов и CSV-файлов, которые могут быть использованы для последующего анализа и визуализации.

3.2 Результаты сценария лёгкой нагрузки (Sanity check)

При лёгкой нагрузке оба сервиса продемонстрировали стабильную работу:

значения RPS соответствовали ожидаемым;

среднее время отклика находилось на низком уровне;

ошибок обработки запросов зафиксировано не было.

gRPC-сервис показал меньшее время отклика по сравнению с REST API, что объясняется использованием бинарной сериализации Protocol Buffers и меньшими накладными расходами при передаче данных.

Данный сценарий подтвердил корректность настройки тестового стенда и возможность перехода к более интенсивным режимам нагрузки.

3.3 Результаты сценария рабочей нагрузки (Normal load)

При рабочей нагрузке наблюдалось увеличение времени отклика по сравнению с sanity check, что является ожидаемым поведением при росте количества одновременных пользователей.

Основные наблюдения:

REST-сервис демонстрировал более заметный рост средней задержки;

gRPC-сервис сохранял более стабильные значения latency;

значения p95 и p99 для gRPC оставались ниже, чем для REST.

Ошибки обработки запросов на данном этапе либо отсутствовали, либо имели единичный характер и не оказывали существенного влияния на общую картину.

3.4 Результаты стресс-теста (Stress test)

В ходе стресс-тестирования нагрузка увеличивалась до значений, превышающих рабочий режим, с целью выявления пределов производительности сервисов.

При увеличении количества пользователей были зафиксированы следующие эффекты:

у REST-сервиса при достижении определённого числа пользователей наблюдался резкий рост времени отклика;

начали появляться ошибки уровня HTTP 5xx, что свидетельствует о перегрузке сервиса;

значения p95 и p99 для REST значительно возросли.

gRPC-сервис показал более устойчивое поведение под высокой нагрузкой. Рост времени отклика происходил более плавно, а момент деградации наступал при большем количестве пользователей по сравнению с REST API.

3.5 Результаты теста на стабильность (Stability test)

Тест на стабильность проводился при умеренной, но длительной нагрузке.

В ходе теста было установлено:

время отклика REST и gRPC сервисов оставалось относительно стабильным;

резкого роста количества ошибок не наблюдалось;

признаков утечек ресурсов или постепенной деградации производительности выявлено не было.

Таким образом, оба сервиса показали устойчивую работу при длительной нагрузке в пределах рабочих параметров.

3.6 Анализ результатов и выявление узких мест

На основании полученных результатов можно сделать следующие выводы:

Момент деградации у REST-сервиса наступает при меньшем количестве пользователей, чем у gRPC;

Рост латентности у REST выражен сильнее и происходит более резко;

gRPC-сервис демонстрирует более плавную деградацию и лучшие показатели p95/p99.

Вероятные причины различий в поведении сервисов:

более высокие накладные расходы сериализации JSON в REST;

использование HTTP/1.1 в REST по сравнению с HTTP/2 в gRPC;

большая вычислительная сложность ресурсоёмких эндпоинтов (например, получение полного графа).

Узкими местами при высокой нагрузке могут являться:

загрузка CPU;

обработка больших объёмов данных;

внутренняя логика формирования ответов. 
Результаты нагрузочного тестирования

В данном разделе представлены результаты нагрузочного тестирования REST и gRPC сервисов, полученные с помощью инструмента Locust. Для каждого сценария фиксировались основные метрики производительности, после чего выполнялся анализ поведения сервисов под различной нагрузкой.

3.1 Основные метрики производительности

В ходе тестирования для каждого сценария были зафиксированы следующие метрики:

RPS (Requests Per Second) — количество запросов в секунду;

Throughput — фактическая пропускная способность сервиса;

Среднее время ответа (Average Response Time);

p95 / p99 latency — 95-й и 99-й перцентили времени отклика;

Количество ошибок (HTTP 5xx, тайм-ауты, ошибки соединения).

Сбор метрик осуществлялся средствами Locust.
Для REST-сервиса использовались стандартные возможности HttpUser, для gRPC-сервиса — ручная регистрация запросов через механизм events.request.fire, что позволило корректно учитывать RPC-вызовы в отчётах Locust.

Результаты тестирования сохранялись в виде HTML-отчётов и CSV-файлов, которые могут быть использованы для последующего анализа и визуализации.

3.2 Результаты сценария лёгкой нагрузки (Sanity check)

При лёгкой нагрузке оба сервиса продемонстрировали стабильную работу:

значения RPS соответствовали ожидаемым;

среднее время отклика находилось на низком уровне;

ошибок обработки запросов зафиксировано не было.

gRPC-сервис показал меньшее время отклика по сравнению с REST API, что объясняется использованием бинарной сериализации Protocol Buffers и меньшими накладными расходами при передаче данных.

Данный сценарий подтвердил корректность настройки тестового стенда и возможность перехода к более интенсивным режимам нагрузки.

3.3 Результаты сценария рабочей нагрузки (Normal load)

При рабочей нагрузке наблюдалось увеличение времени отклика по сравнению с sanity check, что является ожидаемым поведением при росте количества одновременных пользователей.

Основные наблюдения:

REST-сервис демонстрировал более заметный рост средней задержки;

gRPC-сервис сохранял более стабильные значения latency;

значения p95 и p99 для gRPC оставались ниже, чем для REST.

Ошибки обработки запросов на данном этапе либо отсутствовали, либо имели единичный характер и не оказывали существенного влияния на общую картину.

3.4 Результаты стресс-теста (Stress test)

В ходе стресс-тестирования нагрузка увеличивалась до значений, превышающих рабочий режим, с целью выявления пределов производительности сервисов.

При увеличении количества пользователей были зафиксированы следующие эффекты:

у REST-сервиса при достижении определённого числа пользователей наблюдался резкий рост времени отклика;

начали появляться ошибки уровня HTTP 5xx, что свидетельствует о перегрузке сервиса;

значения p95 и p99 для REST значительно возросли.

gRPC-сервис показал более устойчивое поведение под высокой нагрузкой. Рост времени отклика происходил более плавно, а момент деградации наступал при большем количестве пользователей по сравнению с REST API.

3.5 Результаты теста на стабильность (Stability test)

Тест на стабильность проводился при умеренной, но длительной нагрузке.

В ходе теста было установлено:

время отклика REST и gRPC сервисов оставалось относительно стабильным;

резкого роста количества ошибок не наблюдалось;

признаков утечек ресурсов или постепенной деградации производительности выявлено не было.

Таким образом, оба сервиса показали устойчивую работу при длительной нагрузке в пределах рабочих параметров.

3.6 Анализ результатов и выявление узких мест

На основании полученных результатов можно сделать следующие выводы:

Момент деградации у REST-сервиса наступает при меньшем количестве пользователей, чем у gRPC;

Рост латентности у REST выражен сильнее и происходит более резко;

gRPC-сервис демонстрирует более плавную деградацию и лучшие показатели p95/p99.

Вероятные причины различий в поведении сервисов:

более высокие накладные расходы сериализации JSON в REST;

использование HTTP/1.1 в REST по сравнению с HTTP/2 в gRPC;

большая вычислительная сложность ресурсоёмких эндпоинтов (например, получение полного графа).

Узкими местами при высокой нагрузке могут являться:

загрузка CPU;

обработка больших объёмов данных;
### 3.2 Анализ результатов нагрузочного тестирования

#### Момент начала деградации производительности

В ходе стресс-тестирования было установлено, что деградация производительности REST-сервиса начинается при меньшем количестве одновременных пользователей по сравнению с gRPC-сервисом.  
При увеличении нагрузки до высоких значений у REST API наблюдается резкий рост времени отклика и появление ошибок уровня HTTP 5xx, что указывает на перегрузку сервиса.

gRPC-сервис демонстрирует более устойчивое поведение: рост времени отклика происходит более плавно, а деградация наступает при большем количестве пользователей.

---

#### Изменение латентности при росте нагрузки

С увеличением количества пользователей для обоих сервисов наблюдается рост среднего времени отклика и значений p95/p99, что является ожидаемым поведением.

При этом:
- у REST-сервиса рост латентности выражен сильнее и носит более резкий характер;
- у gRPC-сервиса увеличение задержек происходит постепенно, без резких скачков.

Это свидетельствует о более эффективной обработке запросов в gRPC при высокой нагрузке.

---

#### Выявление «бутылочного горлышка»

На основании анализа результатов можно сделать вывод, что основными узкими местами при высокой нагрузке являются:

- загрузка CPU, связанная с обработкой запросов и сериализацией данных;
- вычислительная сложность ресурсоёмких операций (например, получение полного графа или связей);
- накладные расходы выбранного протокола и формата передачи данных.

Влияние сетевых факторов минимально, так как тестирование проводилось на локальном стенде (`localhost`).  
Использование базы данных напрямую не анализировалось, однако при высокой нагрузке её влияние также может усиливать деградацию производительности.

---

#### Сравнение поведения REST и gRPC

Результаты нагрузочного тестирования показывают, что REST и gRPC сервисы демонстрируют различное поведение под нагрузкой:

- REST API быстрее деградирует при росте количества пользователей;
- gRPC-сервис сохраняет более стабильные значения latency и RPS;
- различия объясняются использованием бинарной сериализации Protocol Buffers и протокола HTTP/2 в gRPC, в отличие от JSON и HTTP/1.1 в REST.

Таким образом, gRPC является более устойчивым решением при высокой нагрузке, тогда как REST API проще в реализации, но менее эффективен с точки зрения производительности.
Сравнение REST и gRPC

В данном разделе приведено сравнение производительности REST API и gRPC-сервиса на основе результатов нагрузочного тестирования. Сравнение выполняется по основным метрикам, а также анализируются причины различий в поведении сервисов под нагрузкой.

4.1 Сравнение латентности

Результаты нагрузочного тестирования показали, что gRPC-сервис демонстрирует меньшее время отклика по сравнению с REST API во всех рассмотренных сценариях нагрузки.

Основные наблюдения:

среднее время ответа у gRPC ниже, чем у REST;

значения p95 и p99 для gRPC остаются более стабильными при росте нагрузки;

у REST-сервиса при высокой нагрузке наблюдаются резкие скачки латентности.

Различия в латентности наиболее заметны при выполнении ресурсоёмких операций, таких как получение полного семантического графа или связей между терминами.

4.2 Сравнение пропускной способности (RPS)

При одинаковых параметрах нагрузки gRPC-сервис способен обрабатывать большее количество запросов в секунду по сравнению с REST API.

В ходе тестирования было установлено, что:

REST-сервис быстрее достигает предела пропускной способности;

при дальнейшем увеличении нагрузки RPS у REST начинает снижаться вследствие роста времени отклика и появления ошибок;

gRPC-сервис сохраняет более высокий и стабильный уровень RPS при большем количестве пользователей.

Это указывает на лучшую масштабируемость gRPC-сервиса при высокой нагрузке.

4.3 Анализ накладных расходов (overhead)

Различия в производительности REST и gRPC во многом обусловлены накладными расходами, связанными с передачей и обработкой данных.

Для REST API характерны следующие особенности:

использование текстового формата JSON;

больший объём передаваемых данных;

дополнительные вычислительные затраты на сериализацию и парсинг.

Для gRPC-сервиса:

используется бинарная сериализация Protocol Buffers;

сообщения имеют меньший размер;

применяется протокол HTTP/2, обеспечивающий более эффективное использование соединений.

Указанные различия напрямую влияют на показатели латентности и пропускной способности сервисов при росте нагрузки.

4.4 Выводы о применимости REST и gRPC

На основании проведённого сравнения можно сделать следующие выводы:

REST API является простым и удобным решением для сервисов с умеренной нагрузкой и невысокими требованиями к производительности;

gRPC более эффективен в системах с высокой интенсивностью запросов и большими объёмами передаваемых данных;

при необходимости обеспечения высокой производительности и масштабируемости предпочтительным является использование gRPC.

Таким образом, выбор между REST и gRPC должен осуществляться с учётом характера нагрузки, требований к задержкам и архитектурных особенностей разрабатываемой системы.
Заключение

В рамках данной работы было проведено нагрузочное тестирование и сравнительный анализ двух реализаций приложения-глоссария терминов: REST API на базе FastAPI и gRPC-сервиса с использованием Protocol Buffers. Для проведения эксперимента использовался инструмент Locust, позволивший смоделировать различные профили пользовательской нагрузки и зафиксировать ключевые метрики производительности.

Основные выводы

В результате проведённого тестирования были получены следующие выводы:

gRPC-сервис демонстрирует меньшее время отклика и более стабильные значения p95 и p99 по сравнению с REST API;

REST-сервис быстрее достигает предела производительности при росте количества одновременных пользователей;

при высокой нагрузке у REST API наблюдается резкий рост латентности и появление ошибок, свидетельствующих о деградации сервиса;

gRPC-сервис показывает более плавную деградацию и способен обрабатывать большее количество запросов в секунду.

Различия в поведении сервисов объясняются архитектурными особенностями: использованием бинарной сериализации Protocol Buffers и протокола HTTP/2 в gRPC, а также текстового формата JSON и HTTP/1.1 в REST.

Рекомендации по оптимизации

На основе результатов эксперимента можно предложить следующие рекомендации:

для REST API целесообразно оптимизировать ресурсоёмкие эндпоинты, использовать кэширование и настраивать параметры сервера (количество воркеров, лимиты соединений);

при работе с большими объёмами данных и высокой нагрузкой предпочтительно рассмотреть использование gRPC;

для обоих сервисов рекомендуется использовать мониторинг ресурсов CPU и памяти при нагрузочном тестировании для более точного выявления узких мест.

Возможные улучшения эксперимента

Эксперимент может быть улучшен за счёт следующих доработок:

проведение тестирования в распределённой среде с использованием нескольких узлов;

добавление искусственной сетевой задержки для моделирования реальных условий эксплуатации;

выполнение нескольких повторных прогонов тестов с последующим усреднением результатов;

расширение сценариев нагрузочного тестирования за счёт операций записи.

Ограничения проведённого тестирования

При интерпретации результатов необходимо учитывать следующие ограничения:

тестирование проводилось на локальном стенде без учёта сетевых задержек;

влияние базы данных анализировалось косвенно и не выделялось в отдельную метрику;

мониторинг системных ресурсов не был интегрирован напрямую в отчёты Locust.

Несмотря на указанные ограничения, проведённое нагрузочное тестирование позволяет сделать обоснованные выводы о различиях в производительности REST и gRPC и оценить их применимость в зависимости от требований к системе.
